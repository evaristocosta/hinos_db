{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9388e0d8",
   "metadata": {},
   "source": [
    "# Análise de Embeddings dos Hinos da ICM\n",
    "Este notebook explora técnicas de embeddings para representar os textos dos hinos da coletânea principal da Igreja Cristã Maranata.\n",
    "\n",
    "O objetivo é aplicar modelos de vetorização, calcular similaridade, realizar agrupamentos e extrair tópicos dos louvores, utilizando métodos como FastText, PCA, t-SNE, UMAP, KMeans, LDA e NMF.\n",
    "\n",
    "---\n",
    "**Conteúdo do notebook:**\n",
    "- Carregamento do modelo FastText e dos dados tratados\n",
    "- Geração de embeddings com diferentes estratégias de peso\n",
    "- Cálculo de similaridade entre hinos\n",
    "- Visualização de matrizes de similaridade\n",
    "- Redução de dimensionalidade (PCA, t-SNE, UMAP)\n",
    "- Agrupamento de hinos por KMeans\n",
    "- Extração de tópicos com LDA e NMF\n",
    "- Visualização dos agrupamentos e tópicos\n",
    "- Salvamento dos resultados para uso futuro\n",
    "\n",
    "Este material é público e pode ser compartilhado para fins de pesquisa, estudo ou divulgação cultural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2b65e",
   "metadata": {},
   "source": [
    "# Parte 4 - Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb6887",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Carregamento do modelo FastText pré-treinado para português, utilizado para gerar embeddings das palavras dos hinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e45ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Modelo baixado diretamente do site do fasttext\n",
    "# (https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.bin.gz)\n",
    "model = fasttext.load_model(\"..\\\\assets\\\\cc.pt.300.bin\")\n",
    "\n",
    "# Outra alternativa\n",
    "# fasttext.util.download_model('pt', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b0b18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Carregamento dos dados dos hinos já tokenizados, prontos para análise de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d34746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hinos_analise: pd.DataFrame = pd.read_pickle(\"..\\\\assets\\\\hinos_analise_tokens.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772b715",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Geração dos embeddings dos hinos utilizando diferentes estratégias de peso: TF-IDF, penalização por comprimento e média uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def embed_text_weighted(tokens, model, method=\"tfidf\"):\n",
    "    \"\"\"Embedding com diferentes estratégias de peso\"\"\"\n",
    "    if not tokens:\n",
    "        return np.zeros(model.get_dimension())\n",
    "\n",
    "    vectors = []\n",
    "    weights = []\n",
    "\n",
    "    if method == \"tfidf\":\n",
    "        # Peso baseado em frequência inversa (palavras raras = mais peso)\n",
    "        token_counts = Counter(tokens)\n",
    "        total_docs = len(hinos_analise)  # ou seu corpus total\n",
    "\n",
    "        for word in tokens:\n",
    "            vector = model.get_word_vector(word)\n",
    "            # Simulação simples de TF-IDF\n",
    "            tf = token_counts[word] / len(tokens)\n",
    "            idf = np.log(\n",
    "                total_docs\n",
    "                / (\n",
    "                    1\n",
    "                    + sum(\n",
    "                        1\n",
    "                        for doc_tokens in hinos_analise[\"tokens_no_stops\"]\n",
    "                        if word in doc_tokens\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            weight = tf * idf\n",
    "\n",
    "            vectors.append(vector)\n",
    "            weights.append(weight)\n",
    "\n",
    "    elif method == \"uniform\":\n",
    "        # Sua abordagem atual\n",
    "        vectors = [model.get_word_vector(word) for word in tokens]\n",
    "        weights = [1.0] * len(vectors)\n",
    "\n",
    "    elif method == \"length_penalty\":\n",
    "        # Penaliza documentos muito longos\n",
    "        vectors = [model.get_word_vector(word) for word in tokens]\n",
    "        weights = [1.0 / np.sqrt(len(tokens))] * len(vectors)\n",
    "\n",
    "    # Média ponderada\n",
    "    weighted_sum = np.average(vectors, axis=0, weights=weights)\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "# Teste diferentes abordagens\n",
    "hinos_analise[\"word_embedding_tfidf\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"tfidf\")\n",
    ")\n",
    "\n",
    "hinos_analise[\"word_embedding_length_penalty\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"length_penalty\")\n",
    ")\n",
    "\n",
    "hinos_analise[\"word_embedding\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"uniform\")\n",
    ")\n",
    "hinos_analise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f40ccd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Cálculo da similaridade entre hinos usando os embeddings gerados, comparação dos métodos e exibição dos hinos mais semelhantes ao hino de referência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sims_tfidf = cosine_similarity(list(hinos_analise[\"word_embedding_tfidf\"]))\n",
    "sims_lp = cosine_similarity(list(hinos_analise[\"word_embedding_length_penalty\"]))\n",
    "sims = cosine_similarity(list(hinos_analise[\"word_embedding\"]))\n",
    "\n",
    "# hinos mais semelhantes ao hino 443\n",
    "similarities_tfidf = list(enumerate(sims_tfidf[443]))\n",
    "similarities_tfidf = sorted(similarities_tfidf, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "similarities_lp = list(enumerate(sims_lp[443]))\n",
    "similarities_lp = sorted(similarities_lp, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "similarities = list(enumerate(sims[443]))\n",
    "similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Mais parecidos com o hino 443: \" + hinos_analise[\"nome\"].iloc[443])\n",
    "print(\"TF-IDF:\")\n",
    "for idx, score in similarities_tfidf[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")\n",
    "\n",
    "print(\"Length Penalty:\")\n",
    "for idx, score in similarities_lp[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")\n",
    "\n",
    "print(\"Uniform:\")\n",
    "for idx, score in similarities[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3565f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Criação de DataFrames de similaridade para visualização e análise dos resultados dos diferentes métodos de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_tfidf_df = pd.DataFrame(\n",
    "    sims_tfidf, index=hinos_analise.index, columns=hinos_analise.index\n",
    ")\n",
    "sims_lp_df = pd.DataFrame(\n",
    "    sims_lp, index=hinos_analise.index, columns=hinos_analise.index\n",
    ")\n",
    "sims_df = pd.DataFrame(sims, index=hinos_analise.index, columns=hinos_analise.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa43e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização das matrizes de similaridade entre hinos utilizando heatmaps para os diferentes métodos de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc97ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 5))\n",
    "\n",
    "sns.heatmap(sims_tfidf_df, cmap=\"viridis\", annot=False, ax=ax[0])\n",
    "sns.heatmap(sims_lp_df, cmap=\"viridis\", annot=False, ax=ax[1])\n",
    "sns.heatmap(sims_df, cmap=\"viridis\", annot=False, ax=ax[2])\n",
    "plt.title(\"Similaridade entre hinos (Word Embeddings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e94e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Identificação dos pares de hinos com alta similaridade, destacando possíveis relações temáticas ou estilísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_word2vec = sims_tfidf_df[\n",
    "    (sims_tfidf_df > 0.5) & (sims_tfidf_df < 1.0)\n",
    "].stack()  # .reset_index()\n",
    "high_similarity_word2vec = high_similarity_word2vec[\n",
    "    high_similarity_word2vec.index.get_level_values(0)\n",
    "    < high_similarity_word2vec.index.get_level_values(1)\n",
    "]\n",
    "high_similarity_word2vec.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7b5b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Redução de dimensionalidade dos embeddings dos hinos utilizando PCA, t-SNE e UMAP para visualização e agrupamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419735b1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(\n",
    "    n_components=2,  # 2D\n",
    "    perplexity=30,  # balanceia \"quantos vizinhos\" considerar (20-50 costuma ser bom)\n",
    "    # n_iter=1000,  # número de iterações\n",
    "    random_state=42,\n",
    ")\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=15,  # controla quão “local” é o agrupamento (10–50 bons valores)\n",
    "    min_dist=0.1,  # densidade dos pontos no espaço 2D (0 = pontos bem juntos, 0.5 = mais espalhados)\n",
    "    n_components=2,  # queremos 2D para visualização\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X = np.vstack(hinos_analise[\"word_embedding_tfidf\"].values)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "hinos_analise[\"word_pca1\"] = X_pca[:, 0]\n",
    "hinos_analise[\"word_pca2\"] = X_pca[:, 1]\n",
    "\n",
    "hinos_analise[\"word_tsne1\"] = X_tsne[:, 0]\n",
    "hinos_analise[\"word_tsne2\"] = X_tsne[:, 1]\n",
    "\n",
    "hinos_analise[\"word_umap1\"] = X_umap[:, 0]\n",
    "hinos_analise[\"word_umap2\"] = X_umap[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d92be2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização dos agrupamentos dos hinos por embeddings reduzidos, utilizando scatterplots para PCA, t-SNE e UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Criar figura com 3 subplots lado a lado\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot PCA\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_pca1\",\n",
    "    y=\"word_pca2\",\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"PCA - Hinos agrupados por embeddings\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot t-SNE\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_tsne1\",\n",
    "    y=\"word_tsne2\",\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"t-SNE - Hinos agrupados por embeddings\")\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot UMAP\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_umap1\",\n",
    "    y=\"word_umap2\",\n",
    "    ax=axes[2],\n",
    ")\n",
    "axes[2].set_title(\"UMAP - Hinos agrupados por embeddings\")\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f9c9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Análise de agrupamento dos hinos utilizando KMeans, avaliação do número ideal de clusters com o coeficiente de Silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "range_n_clusters = range(2, 12)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_umap)\n",
    "    score = silhouette_score(X_umap, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k = {k}, silhouette = {score:.4f}\")\n",
    "\n",
    "# Visualiza o resultado\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker=\"o\")\n",
    "plt.title(\"Análise de Silhouette para seleção de k\")\n",
    "plt.xlabel(\"Número de clusters (k)\")\n",
    "plt.ylabel(\"Coeficiente médio de Silhouette\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e37e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização dos coeficientes de Silhouette para diferentes valores de k, auxiliando na escolha do número de clusters ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "k = 10  # exemplo, número escolhido\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(X_umap)\n",
    "\n",
    "silhouette_vals = silhouette_samples(X_umap, labels)\n",
    "y_lower = 10\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(k):\n",
    "    ith_vals = silhouette_vals[labels == i]\n",
    "    ith_vals.sort()\n",
    "    size_cluster_i = ith_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_vals)\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "plt.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"Silhouette plot para k={k}\")\n",
    "plt.xlabel(\"Coeficiente de Silhouette\")\n",
    "plt.ylabel(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bef73d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização dos coeficientes de Silhouette por cluster, detalhando a qualidade dos agrupamentos formados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a24bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "hinos_analise[\"word_cluster\"] = kmeans.fit_predict(X_umap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd9414",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Atribuição dos clusters aos hinos e visualização dos agrupamentos por embeddings reduzidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Criar figura com 3 subplots lado a lado\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot PCA\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_umap1\",\n",
    "    y=\"word_umap2\",\n",
    "    hue=\"word_cluster\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    ")\n",
    "plt.title(\"PCA - Hinos agrupados por embeddings\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e53f80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Análise dos termos mais frequentes em cada cluster e visualização da distribuição dos hinos por cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65427d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for c in sorted(hinos_analise[\"word_cluster\"].unique()):\n",
    "    cluster_tokens = hinos_analise.loc[\n",
    "        hinos_analise[\"word_cluster\"] == c, \"tokens_no_stops\"\n",
    "    ].sum()\n",
    "    top_terms = Counter(cluster_tokens).most_common(10)\n",
    "    print(f\"\\nCluster {c}:\")\n",
    "    print([t for t, _ in top_terms])\n",
    "    print(hinos_analise.loc[hinos_analise[\"word_cluster\"] == c, \"nome\"][:5])\n",
    "\n",
    "print(hinos_analise[\"word_cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce606d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Extração de tópicos dos hinos utilizando LDA e NMF, atribuição dos tópicos aos hinos e visualização da distribuição dos tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c5b9c",
   "metadata": {},
   "source": [
    "## Tópicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293823d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização dos agrupamentos dos hinos por tópicos, utilizando scatterplots para PCA, t-SNE e UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc90bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "n_topics = n_clusters\n",
    "\n",
    "# Criar TF-IDF apenas para análise de tópicos\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    stop_words=None,  # você já removeu as stopwords\n",
    "    ngram_range=(1, 3),  # uni, bi e trigramas\n",
    "    min_df=2,  # palavra deve aparecer em pelo menos 2 documentos\n",
    ")\n",
    "\n",
    "# Usar texto já limpo (sem stopwords)\n",
    "texts_for_topics = [\" \".join(tokens) for tokens in hinos_analise[\"tokens_no_stops\"]]\n",
    "X_tfidf = vectorizer.fit_transform(texts_for_topics)\n",
    "\n",
    "# Agora podemos usar LDA e NMF\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_topics = lda.fit_transform(X_tfidf)\n",
    "\n",
    "# NMF também funciona com TF-IDF\n",
    "nmf = NMF(n_components=n_topics, random_state=42, max_iter=100)\n",
    "nmf_topics = nmf.fit_transform(X_tfidf)\n",
    "\n",
    "\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTópico {idx+1}:\")\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n",
    "        print(f\"Palavras-chave: {' | '.join(top_words)}\")\n",
    "\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=== LDA (Latent Dirichlet Allocation) ===\")\n",
    "display_topics(lda, feature_names)\n",
    "\n",
    "print(\"\\n=== NMF (Non-negative Matrix Factorization) ===\")\n",
    "display_topics(nmf, feature_names)\n",
    "\n",
    "# Atribuir tópicos aos hinos\n",
    "hinos_analise[\"LDA_topic\"] = lda_topics.argmax(axis=1)\n",
    "hinos_analise[\"NMF_topic\"] = nmf_topics.argmax(axis=1)\n",
    "\n",
    "print(f\"\\nDistribuição LDA:\")\n",
    "print(hinos_analise[\"LDA_topic\"].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDistribuição NMF:\")\n",
    "print(hinos_analise[\"NMF_topic\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f764a9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização da distribuição dos tópicos atribuídos aos hinos por LDA e NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição de tópicos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# LDA\n",
    "hinos_analise[\"LDA_topic\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", ax=axes[0], title=\"LDA (TF-IDF)\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Tópico\")\n",
    "axes[0].set_ylabel(\"Número de Hinos\")\n",
    "\n",
    "# NMF\n",
    "hinos_analise[\"NMF_topic\"].value_counts().sort_index().plot(\n",
    "    kind=\"bar\", ax=axes[1], title=\"NMF (TF-IDF)\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Tópico\")\n",
    "axes[1].set_ylabel(\"Número de Hinos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8430da",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Visualização dos agrupamentos dos hinos por tópicos em diferentes projeções (PCA, t-SNE, UMAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar figura com 3 subplots lado a lado\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot PCA\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_pca1\",\n",
    "    y=\"word_pca2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"PCA - Hinos agrupados por tópicos\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot t-SNE\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_tsne1\",\n",
    "    y=\"word_tsne2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"t-SNE - Hinos agrupados por tópicos\")\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot UMAP\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"word_umap1\",\n",
    "    y=\"word_umap2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[2],\n",
    ")\n",
    "axes[2].set_title(\"UMAP - Hinos agrupados por tópicos\")\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f46d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** Salvamento dos resultados e informações enriquecidas dos hinos para uso em análises futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b3788",
   "metadata": {},
   "source": [
    "## Salvamento de informações novas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7344ae2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Fim do notebook:** Finalização do processamento, com os dados prontos para exportação e uso em outras análises ou aplicações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinos_analise[\n",
    "    [\n",
    "        \"nome\",\n",
    "        \"texto_limpo\",\n",
    "        \"categoria_id\",\n",
    "        \"categoria_abr\",\n",
    "        \"tokens_no_stops\",\n",
    "        \"word_embedding_tfidf\",\n",
    "        \"word_tsne1\",\n",
    "        \"word_tsne2\",\n",
    "        \"word_umap1\",\n",
    "        \"word_umap2\",\n",
    "        \"word_cluster\",\n",
    "        \"NMF_topic\",\n",
    "    ]\n",
    "].to_pickle(\"..\\\\assets\\\\hinos_analise_word_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec214221",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_tfidf_df.to_pickle(\"..\\\\assets\\\\similarity_matrix_word_embeddings_tfidf.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
