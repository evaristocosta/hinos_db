{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c2b65e",
   "metadata": {},
   "source": [
    "# Parte 4 - Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# fasttext.util.download_model('pt', if_exists='ignore')\n",
    "model = fasttext.load_model(\"..\\\\assets\\\\cc.pt.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d34746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hinos_analise:pd.DataFrame = pd.read_pickle(\"..\\\\assets\\\\hinos_analise_tokens.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def embed_text_weighted(tokens, model, method=\"tfidf\"):\n",
    "    \"\"\"Embedding com diferentes estratégias de peso\"\"\"\n",
    "    if not tokens:\n",
    "        return np.zeros(model.get_dimension())\n",
    "\n",
    "    vectors = []\n",
    "    weights = []\n",
    "\n",
    "    if method == \"tfidf\":\n",
    "        # Peso baseado em frequência inversa (palavras raras = mais peso)\n",
    "        token_counts = Counter(tokens)\n",
    "        total_docs = len(hinos_analise)  # ou seu corpus total\n",
    "\n",
    "        for word in tokens:\n",
    "            vector = model.get_word_vector(word)\n",
    "            # Simulação simples de TF-IDF\n",
    "            tf = token_counts[word] / len(tokens)\n",
    "            idf = np.log(\n",
    "                total_docs\n",
    "                / (\n",
    "                    1\n",
    "                    + sum(\n",
    "                        1\n",
    "                        for doc_tokens in hinos_analise[\"tokens_no_stops\"]\n",
    "                        if word in doc_tokens\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            weight = tf * idf\n",
    "\n",
    "            vectors.append(vector)\n",
    "            weights.append(weight)\n",
    "\n",
    "    elif method == \"uniform\":\n",
    "        # Sua abordagem atual\n",
    "        vectors = [model.get_word_vector(word) for word in tokens]\n",
    "        weights = [1.0] * len(vectors)\n",
    "\n",
    "    elif method == \"length_penalty\":\n",
    "        # Penaliza documentos muito longos\n",
    "        vectors = [model.get_word_vector(word) for word in tokens]\n",
    "        weights = [1.0 / np.sqrt(len(tokens))] * len(vectors)\n",
    "\n",
    "    # Média ponderada\n",
    "    weighted_sum = np.average(vectors, axis=0, weights=weights)\n",
    "    return weighted_sum\n",
    "\n",
    "\n",
    "# Teste diferentes abordagens\n",
    "hinos_analise[\"embedding_tfidf\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"tfidf\")\n",
    ")\n",
    "\n",
    "hinos_analise[\"embedding_length_penalty\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"length_penalty\")\n",
    ")\n",
    "\n",
    "hinos_analise[\"embedding\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: embed_text_weighted(t, model, \"uniform\")\n",
    ")\n",
    "hinos_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sims_tfidf = cosine_similarity(list(hinos_analise[\"embedding_tfidf\"]))\n",
    "sims_lp = cosine_similarity(list(hinos_analise[\"embedding_length_penalty\"]))\n",
    "sims = cosine_similarity(list(hinos_analise[\"embedding\"]))\n",
    "\n",
    "# hinos mais semelhantes ao hino 443\n",
    "similarities_tfidf = list(enumerate(sims_tfidf[443]))\n",
    "similarities_tfidf = sorted(similarities_tfidf, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "similarities_lp = list(enumerate(sims_lp[443]))\n",
    "similarities_lp = sorted(similarities_lp, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "similarities = list(enumerate(sims[443]))\n",
    "similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Mais parecidos com o hino 443: \" + hinos_analise[\"nome\"].iloc[443])\n",
    "print(\"TF-IDF:\")\n",
    "for idx, score in similarities_tfidf[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")\n",
    "\n",
    "print(\"Length Penalty:\")\n",
    "for idx, score in similarities_lp[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")\n",
    "\n",
    "print(\"Uniform:\")\n",
    "for idx, score in similarities[1:6]:\n",
    "    print(f\"Hino {idx}: {hinos_analise['nome'].iloc[idx]} → similaridade {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_tfidf_df = pd.DataFrame(sims_tfidf, index=hinos_analise.index, columns=hinos_analise.index)\n",
    "sims_lp_df = pd.DataFrame(sims_lp, index=hinos_analise.index, columns=hinos_analise.index)\n",
    "sims_df = pd.DataFrame(sims, index=hinos_analise.index, columns=hinos_analise.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc97ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(24, 5))\n",
    "\n",
    "sns.heatmap(sims_tfidf_df, cmap=\"viridis\", annot=False, ax=ax[0])\n",
    "sns.heatmap(sims_lp_df, cmap=\"viridis\", annot=False, ax=ax[1])\n",
    "sns.heatmap(sims_df, cmap=\"viridis\", annot=False, ax=ax[2])\n",
    "plt.title(\"Similaridade entre hinos (Word Embeddings)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_word2vec = sims[\n",
    "    (sims > 0.5) & (sims < 1.0)\n",
    "].stack()  # .reset_index()\n",
    "high_similarity_word2vec = high_similarity_word2vec[\n",
    "    high_similarity_word2vec.index.get_level_values(0)\n",
    "    < high_similarity_word2vec.index.get_level_values(1)\n",
    "]\n",
    "high_similarity_word2vec.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419735b1",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "tsne = TSNE(\n",
    "    n_components=2,  # 2D\n",
    "    perplexity=30,  # balanceia \"quantos vizinhos\" considerar (20-50 costuma ser bom)\n",
    "    #n_iter=1000,  # número de iterações\n",
    "    random_state=42,\n",
    ")\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=15,  # controla quão “local” é o agrupamento (10–50 bons valores)\n",
    "    min_dist=0.1,  # densidade dos pontos no espaço 2D (0 = pontos bem juntos, 0.5 = mais espalhados)\n",
    "    n_components=2,  # queremos 2D para visualização\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "X = np.vstack(hinos_analise[\"embedding\"].values)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "X_umap = umap_model.fit_transform(X)\n",
    "\n",
    "hinos_analise[\"pca1\"] = X_pca[:, 0]\n",
    "hinos_analise[\"pca2\"] = X_pca[:, 1]\n",
    "\n",
    "hinos_analise[\"tsne1\"] = X_tsne[:, 0]\n",
    "hinos_analise[\"tsne2\"] = X_tsne[:, 1]\n",
    "\n",
    "hinos_analise[\"umap1\"] = X_umap[:, 0]\n",
    "hinos_analise[\"umap2\"] = X_umap[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd52d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# X é sua matriz vetorial (TF-IDF, embeddings etc.)\n",
    "# Exemplo: X = tfidf.fit_transform(df[\"texto\"])  ou  X = np.array(list(df[\"embeddings\"]))\n",
    "\n",
    "range_n_clusters = range(2, 12)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_tsne)\n",
    "    score = silhouette_score(X_tsne, labels)\n",
    "    silhouette_scores.append(score)\n",
    "    print(f\"k = {k}, silhouette = {score:.4f}\")\n",
    "\n",
    "# Visualiza o resultado\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range_n_clusters, silhouette_scores, marker=\"o\")\n",
    "plt.title(\"Análise de Silhouette para seleção de k\")\n",
    "plt.xlabel(\"Número de clusters (k)\")\n",
    "plt.ylabel(\"Coeficiente médio de Silhouette\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a66ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "k = 9  # exemplo, número escolhido\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "labels = kmeans.fit_predict(X_tsne)\n",
    "\n",
    "silhouette_vals = silhouette_samples(X_tsne, labels)\n",
    "y_lower = 10\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(k):\n",
    "    ith_vals = silhouette_vals[labels == i]\n",
    "    ith_vals.sort()\n",
    "    size_cluster_i = ith_vals.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "    plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_vals)\n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "plt.axvline(x=np.mean(silhouette_vals), color=\"red\", linestyle=\"--\")\n",
    "plt.title(f\"Silhouette plot para k={k}\")\n",
    "plt.xlabel(\"Coeficiente de Silhouette\")\n",
    "plt.ylabel(\"Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a24bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# n_clusters = len(hinos_analise[\"categoria_id\"].unique())\n",
    "n_clusters = 9\n",
    "# número de clusters (experimente, ex.: 4 ou 6)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "hinos_analise[\"cluster\"] = kmeans.fit_predict(X)\n",
    "\n",
    "agg_clust = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "hinos_analise[\"agg_cluster\"] = agg_clust.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Criar figura com 3 subplots lado a lado\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot PCA\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise, x=\"pca1\", y=\"pca2\", hue=\"cluster\", palette=\"tab10\", s=80, ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"PCA - Hinos agrupados por embeddings\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot t-SNE\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise, x=\"tsne1\", y=\"tsne2\", hue=\"cluster\", palette=\"tab10\", s=80, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"t-SNE - Hinos agrupados por embeddings\")\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot UMAP\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise, x=\"umap1\", y=\"umap2\", hue=\"cluster\", palette=\"tab10\", s=80, ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"UMAP - Hinos agrupados por embeddings\")\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65427d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for c in sorted(hinos_analise[\"cluster\"].unique()):\n",
    "    cluster_tokens = hinos_analise.loc[hinos_analise[\"cluster\"] == c, \"tokens_no_stops\"].sum()\n",
    "    top_terms = Counter(cluster_tokens).most_common(10)\n",
    "    print(f\"\\nCluster {c}:\")\n",
    "    print([t for t, _ in top_terms])\n",
    "    print(hinos_analise.loc[hinos_analise[\"cluster\"] == c, \"nome\"][:5])\n",
    "\n",
    "print(hinos_analise[\"cluster\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c5b9c",
   "metadata": {},
   "source": [
    "## Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc90bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "n_topics = n_clusters\n",
    "\n",
    "# Criar TF-IDF apenas para análise de tópicos\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500, \n",
    "    stop_words=None,  # você já removeu as stopwords\n",
    "    ngram_range=(1, 2),  # uni e bigramas\n",
    "    min_df=2  # palavra deve aparecer em pelo menos 2 documentos\n",
    ")\n",
    "\n",
    "# Usar texto já limpo (sem stopwords)\n",
    "texts_for_topics = [' '.join(tokens) for tokens in hinos_analise['tokens_no_stops']]\n",
    "X_tfidf = vectorizer.fit_transform(texts_for_topics)\n",
    "\n",
    "# Agora podemos usar LDA e NMF\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10)\n",
    "lda_topics = lda.fit_transform(X_tfidf)\n",
    "\n",
    "# NMF também funciona com TF-IDF\n",
    "nmf = NMF(n_components=n_topics, random_state=42, max_iter=100)\n",
    "nmf_topics = nmf.fit_transform(X_tfidf)\n",
    "\n",
    "# Função original adaptada\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTópico {idx+1}:\")\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n",
    "        print(f\"Palavras-chave: {' | '.join(top_words)}\")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"=== LDA (Latent Dirichlet Allocation) ===\")\n",
    "display_topics(lda, feature_names)\n",
    "\n",
    "print(\"\\n=== NMF (Non-negative Matrix Factorization) ===\")\n",
    "display_topics(nmf, feature_names)\n",
    "\n",
    "# Atribuir tópicos aos hinos\n",
    "hinos_analise[\"LDA_topic\"] = lda_topics.argmax(axis=1)\n",
    "hinos_analise[\"NMF_topic\"] = nmf_topics.argmax(axis=1)\n",
    "\n",
    "print(f\"\\nDistribuição LDA:\")\n",
    "print(hinos_analise[\"LDA_topic\"].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nDistribuição NMF:\")\n",
    "print(hinos_analise[\"NMF_topic\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar a distribuição de tópicos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# LDA\n",
    "hinos_analise['LDA_topic'].value_counts().sort_index().plot(kind='bar', ax=axes[0], title='LDA (TF-IDF)')\n",
    "axes[0].set_xlabel('Tópico')\n",
    "axes[0].set_ylabel('Número de Hinos')\n",
    "\n",
    "# NMF\n",
    "hinos_analise['NMF_topic'].value_counts().sort_index().plot(kind='bar', ax=axes[1], title='NMF (TF-IDF)')\n",
    "axes[1].set_xlabel('Tópico')\n",
    "axes[1].set_ylabel('Número de Hinos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar figura com 3 subplots lado a lado\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot PCA\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"pca1\",\n",
    "    y=\"pca2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set_title(\"PCA - Hinos agrupados por embeddings\")\n",
    "axes[0].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot t-SNE\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"tsne1\",\n",
    "    y=\"tsne2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set_title(\"t-SNE - Hinos agrupados por embeddings\")\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# Plot UMAP\n",
    "sns.scatterplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"umap1\",\n",
    "    y=\"umap2\",\n",
    "    hue=\"NMF_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=80,\n",
    "    ax=axes[2],\n",
    ")\n",
    "axes[2].set_title(\"UMAP - Hinos agrupados por embeddings\")\n",
    "axes[2].legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
