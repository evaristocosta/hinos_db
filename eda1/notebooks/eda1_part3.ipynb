{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114e7f6d",
   "metadata": {},
   "source": [
    "# EDA 1 | Parte 3 - AnÃ¡lise dos Textos dos Hinos da ICM\n",
    "Este notebook explora o conteÃºdo textual dos hinos da coletÃ¢nea principal da Igreja CristÃ£ Maranata.\n",
    "\n",
    "O objetivo Ã© analisar o texto dos louvores, extraindo informaÃ§Ãµes como nÃºmero de palavras, tokens, frequÃªncias, n-grams e similaridade entre hinos.\n",
    "\n",
    "---\n",
    "**ConteÃºdo do notebook:**\n",
    "- Carregamento dos dados tratados\n",
    "- TokenizaÃ§Ã£o e remoÃ§Ã£o de stopwords\n",
    "- AnÃ¡lise do nÃºmero de palavras por hino\n",
    "- VisualizaÃ§Ã£o da distribuiÃ§Ã£o de tokens por categoria\n",
    "- AnÃ¡lise de palavras mais longas e frequentes\n",
    "- GeraÃ§Ã£o de nuvem de palavras\n",
    "- ExtraÃ§Ã£o de n-grams (bigramas, trigramas)\n",
    "- CÃ¡lculo de similaridade entre hinos (CountVectorizer e TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132901ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** ImportaÃ§Ã£o e preparaÃ§Ã£o da lista de stopwords, combinando fontes externas e do NLTK para uso na tokenizaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811adb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "assets_folder = Path(\"../assets\")\n",
    "hinos_analise: pd.DataFrame = pd.read_pickle(assets_folder / \"hinos_analise.pkl\")\n",
    "hinos_analise = hinos_analise.set_index(\"numero\")\n",
    "hinos_analise[\"categoria_abr\"] = hinos_analise[\"categoria\"].apply(\n",
    "    lambda x: x[:13] + \"...\" if len(x) > 15 else x\n",
    ")\n",
    "hinos_analise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94777ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TokenizaÃ§Ã£o\n",
    "\n",
    "**A seguir:** TokenizaÃ§Ã£o dos textos dos hinos, remoÃ§Ã£o de stopwords e pontuaÃ§Ã£o, e cÃ¡lculo do nÃºmero total de palavras por hino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b288c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords_nltk = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "\n",
    "with open(assets_folder / \"stopwords-br.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "# remover linhas que comecao com #\n",
    "stopwords = [eval(word) for word in stopwords if not word.startswith(\"#\")]\n",
    "stopwords.extend([\"Ã³\", \"ti\", \"pra\", \"lo\", \"oh\", \"Ã©s\"])\n",
    "\n",
    "# merge\n",
    "stopwords = list(set(stopwords + stopwords_nltk))\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c3f8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** ExibiÃ§Ã£o dos 10 hinos com maior e menor quantidade de palavras, destacando extremos do corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "all_tokens = []\n",
    "all_tokens_no_stops = []\n",
    "\n",
    "for hino in tqdm(hinos_analise.to_dict(\"records\")):\n",
    "    tokens = nltk.tokenize.regexp_tokenize(hino[\"texto_limpo\"], r\"\\w+\")\n",
    "    # Replace \"MINH\" com \"MINHA\" usando regex\n",
    "    tokens = [nltk.re.sub(r\"^minh$\", \"minha\", palavra.lower()) for palavra in tokens]\n",
    "    tokens_no_stops = [\n",
    "        palavra for palavra in tokens if palavra.lower() not in stopwords\n",
    "    ]\n",
    "    # remover pontuacao\n",
    "    tokens = [palavra for palavra in tokens if palavra.isalpha()]\n",
    "    tokens_no_stops = [palavra for palavra in tokens_no_stops if palavra.isalpha()]\n",
    "\n",
    "    all_tokens.append(tokens)\n",
    "    all_tokens_no_stops.append(tokens_no_stops)\n",
    "\n",
    "hinos_analise[\"tokens\"] = all_tokens\n",
    "hinos_analise[\"tokens_no_stops\"] = all_tokens_no_stops\n",
    "# considerando numero total de palavras, pois todas elas tem que ser cantadas, logo impactam no tamanho prÃ¡tico do hino\n",
    "hinos_analise[\"num_tokens\"] = hinos_analise[\"tokens\"].apply(len)\n",
    "hinos_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hinos_analise.sort_values(\"num_tokens\", ascending=False).head(10))\n",
    "display(hinos_analise.sort_values(\"num_tokens\", ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398925b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** VisualizaÃ§Ã£o da distribuiÃ§Ã£o do nÃºmero de palavras por categoria, utilizando boxplot para identificar padrÃµes e variaÃ§Ãµes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755eb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'categoria_id' is treated as a categorical variable\n",
    "hinos_analise[\"categoria_id\"] = hinos_analise[\"categoria_id\"].astype(\"category\")\n",
    "\n",
    "# Create a mapping between categoria_id and categoria\n",
    "categoria_mapping = (\n",
    "    hinos_analise[[\"categoria_id\", \"categoria_abr\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"categoria_id\")[\"categoria_abr\"]\n",
    ")\n",
    "\n",
    "# Create a Boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=hinos_analise, x=\"categoria_id\", y=\"num_tokens\", palette=\"viridis\")\n",
    "\n",
    "# Replace x-ticks with corresponding 'categoria' names\n",
    "plt.xticks(\n",
    "    ticks=range(len(categoria_mapping)),\n",
    "    labels=categoria_mapping,\n",
    "    rotation=90,\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Categoria\")\n",
    "plt.ylabel(\"Number of Tokens\")\n",
    "plt.title(\"Relationship Between Number of Tokens and Categoria (Box Plot)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4e213",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** AnÃ¡lise de palavras: extraÃ§Ã£o, identificaÃ§Ã£o das mais longas, contagem de frequÃªncia e visualizaÃ§Ã£o das palavras mais comuns com nuvem de palavras e grÃ¡fico de barras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418aef02",
   "metadata": {},
   "source": [
    "## Histograma de frequÃªncia de tamanho das palavras\n",
    "\n",
    "**A seguir:** GeraÃ§Ã£o de um histograma mostrando a frequÃªncia dos tamanhos das palavras nos textos dos hinos, para entender a distribuiÃ§Ã£o do comprimento das palavras utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [\n",
    "    len(t_line) for t_line in hinos_analise[\"tokens_no_stops\"].explode().tolist()\n",
    "]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211aa20e",
   "metadata": {},
   "source": [
    "## Palavras mais longas\n",
    "\n",
    "**A seguir:** ExibiÃ§Ã£o das 10 palavras mais longas encontradas nos textos dos hinos, destacando a diversidade lexical do corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = hinos_analise[\"tokens_no_stops\"].explode().tolist()\n",
    "\n",
    "# find the 10 largest words\n",
    "palavras_unique = list(set(palavras))\n",
    "palavras_unique.sort(key=len, reverse=True)\n",
    "print(\"Quantidade de palavras Ãºnicas (sem repetiÃ§Ã£o):\",len(palavras_unique))\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"palavra\": palavras_unique[:10],\n",
    "        \"tamanho\": [len(palavra) for palavra in palavras_unique[:10]],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73d61b",
   "metadata": {},
   "source": [
    "## Bag-of-words\n",
    "\n",
    "**A seguir:** CriaÃ§Ã£o de uma representaÃ§Ã£o bag-of-words dos textos dos hinos, incluindo a frequÃªncia de cada palavra e um mapeamento das palavras para seus Ã­ndices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total de palavras:\",len(palavras))\n",
    "set_words_full = list(set(palavras))\n",
    "count_words = [palavras.count(i) for i in set_words_full]\n",
    "\n",
    "contagem_palav = pd.DataFrame(\n",
    "    zip(set_words_full, count_words), columns=[\"palavra\", \"contagem\"]\n",
    ")\n",
    "contagem_palav = contagem_palav.sort_values(\"contagem\", ascending=False)\n",
    "contagem_palav[\"percentual\"] = contagem_palav[\"contagem\"] / len(palavras) * 100\n",
    "contagem_palav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61056671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Also show top 20 most frequent words as a bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = contagem_palav.head(20)\n",
    "plt.barh(range(len(top_20)), top_20[\"contagem\"], color=\"skyblue\")\n",
    "plt.yticks(range(len(top_20)), top_20[\"palavra\"])\n",
    "plt.xlabel(\"FrequÃªncia\")\n",
    "plt.title(\"Top 20 Palavras Mais Frequentes\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac001e",
   "metadata": {},
   "source": [
    "## Word cloud\n",
    "\n",
    "**A seguir:** GeraÃ§Ã£o de uma nuvem de palavras para visualizar as palavras mais frequentes nos textos dos hinos, destacando termos recorrentes e temas predominantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba23669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a dictionary from the word frequency data\n",
    "word_freq_dict = dict(zip(contagem_palav[\"palavra\"], contagem_palav[\"contagem\"]))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color=\"white\",\n",
    "    max_words=100,\n",
    "    colormap=\"viridis\",\n",
    "    relative_scaling=0.5,\n",
    "    random_state=42,\n",
    ").generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud - Palavras mais frequentes nos hinos\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f2f13",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2fd89",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** ExtraÃ§Ã£o e anÃ¡lise de n-grams (bigramas e trigramas) para identificar padrÃµes de palavras recorrentes nos textos dos hinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Exemplo: gerar bigramas do corpus inteiro\n",
    "def get_bigrams(tokens):\n",
    "    return list(nltk.ngrams(tokens, 2))  # 2 = bigramas\n",
    "\n",
    "\n",
    "# Gerar bigramas para todos os hinos\n",
    "hinos_analise[\"bigrams\"] = hinos_analise[\"tokens_no_stops\"].apply(get_bigrams)\n",
    "\n",
    "# Contar bigramas mais frequentes no corpus inteiro\n",
    "all_bigrams = [bigram for hino in hinos_analise[\"bigrams\"] for bigram in hino]\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "\n",
    "bigram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar trigramas do corpus inteiro\n",
    "def get_trigrams(tokens):\n",
    "    return list(nltk.ngrams(tokens, 3))  # 3 = trigrams\n",
    "\n",
    "\n",
    "# Gerar trigrams para todos os hinos\n",
    "hinos_analise[\"trigrams\"] = hinos_analise[\"tokens_no_stops\"].apply(get_trigrams)\n",
    "\n",
    "# Contar trigrams mais frequentes no corpus inteiro\n",
    "all_trigrams = [trigram for hino in hinos_analise[\"trigrams\"] for trigram in hino]\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "trigram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224fead",
   "metadata": {},
   "source": [
    "## Matriz de frequÃªncia e similaridade CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757cdfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** CÃ¡lculo de matriz de similaridade entre hinos usando CountVectorizer, visualizaÃ§Ã£o com heatmap e identificaÃ§Ã£o de hinos similares por esse mÃ©todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Juntar os tokens em strings\n",
    "hinos_analise[\"tokens_str\"] = hinos_analise[\"tokens_no_stops\"].apply(\n",
    "    lambda t: \" \".join(t)\n",
    ")\n",
    "\n",
    "# Criar o vetor de frequÃªncias\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3), stop_words=None\n",
    ")  # unigramas, bigramas e trigramas\n",
    "X = vectorizer.fit_transform(hinos_analise[\"tokens_str\"])\n",
    "\n",
    "# Similaridade de cosseno entre hinos\n",
    "similarity = cosine_similarity(X)\n",
    "similarity_df = pd.DataFrame(\n",
    "    similarity, index=hinos_analise.index, columns=hinos_analise.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11effc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df, cmap=\"viridis\", annot=False)\n",
    "plt.title(\"Similaridade entre hinos\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b082682",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity = similarity_df[\n",
    "    (similarity_df > 0.5) & (similarity_df < 1.0)\n",
    "].stack()  # .reset_index()\n",
    "high_similarity = high_similarity[\n",
    "    high_similarity.index.get_level_values(0)\n",
    "    < high_similarity.index.get_level_values(1)\n",
    "]\n",
    "high_similarity.sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204983c6",
   "metadata": {},
   "source": [
    "## Matriz com TF-IDF\n",
    "\n",
    "**A seguir:** CÃ¡lculo de matriz de similaridade entre hinos usando TF-IDF, visualizaÃ§Ã£o com heatmap e identificaÃ§Ã£o de hinos similares por esse mÃ©todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF: unigrams e bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=None)\n",
    "X_tfidf = vectorizer.fit_transform(hinos_analise[\"tokens_str\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5272ab",
   "metadata": {},
   "source": [
    "**A seguir:** Ranking de termos mais relevantes por hino com TF-IDF, destacando palavras-chave que caracterizam cada hino individualmente (uni e bigramas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c828bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def top_terms_for_hymn(row, features, top_n=5):\n",
    "    row_data = list(zip(features, row))\n",
    "    row_data = sorted(row_data, key=lambda x: x[1], reverse=True)\n",
    "    return row_data[:top_n]\n",
    "\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# random.seed(42)\n",
    "sample_idxs = random.sample(range(X_tfidf.shape[0]), 3)\n",
    "\n",
    "for idx in sample_idxs:\n",
    "    row = X_tfidf[idx].toarray().ravel()\n",
    "    top_terms = top_terms_for_hymn(row, features, top_n=3)\n",
    "    hymn_num = hinos_analise.index[idx]\n",
    "    hymn_name = hinos_analise.loc[hymn_num, \"nome\"]\n",
    "    print(f\"\\nðŸŽµ Hino {hymn_num} â€” {hymn_name}:\")\n",
    "    for term, score in top_terms:\n",
    "        print(f\"  {term}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac536814",
   "metadata": {},
   "source": [
    "**A seguir:** Construindo a matriz de similaridade entre hinos usando TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_tfidf = cosine_similarity(X_tfidf)\n",
    "similarity_df_tfidf = pd.DataFrame(\n",
    "    similarity_tfidf, index=hinos_analise.index, columns=hinos_analise.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0556c0a",
   "metadata": {},
   "source": [
    "**A seguir:** VisualizaÃ§Ã£o com heatmap e identificaÃ§Ã£o de hinos similares por esse mÃ©todo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df_tfidf, cmap=\"viridis\", annot=False)\n",
    "plt.title(\"Similaridade entre hinos (TF-IDF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a92e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_tfidf = similarity_df_tfidf[\n",
    "    (similarity_df_tfidf > 0.5) & (similarity_df_tfidf < 1.0)\n",
    "].stack()  # .reset_index()\n",
    "high_similarity_tfidf = high_similarity_tfidf[\n",
    "    high_similarity_tfidf.index.get_level_values(0)\n",
    "    < high_similarity_tfidf.index.get_level_values(1)\n",
    "]\n",
    "high_similarity_tfidf.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830c55a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**A seguir:** ExportaÃ§Ã£o dos dados tratados e enriquecidos para arquivo pickle, permitindo reutilizaÃ§Ã£o em outras anÃ¡lises ou notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c09f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinos_analise.to_pickle(assets_folder / \"hinos_analise_tokens.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
