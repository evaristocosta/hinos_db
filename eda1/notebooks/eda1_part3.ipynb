{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609abdba",
   "metadata": {},
   "source": [
    "# Part 3 - texto\n",
    "\n",
    "- maior/menor louvor\n",
    "- word length (maybe)\n",
    "- bag-of-words with frequency\n",
    "- gensim corpus\n",
    "- tf-idf\n",
    "- named entity recognition (spacy or polyglot)\n",
    "- next: category classification for avulsos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d72b2",
   "metadata": {},
   "source": [
    "## Maior e menor louvor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811adb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "assets_folder = Path(\"../assets\")\n",
    "hinos_analise = pd.read_pickle(assets_folder / \"hinos_analise.pkl\")\n",
    "hinos_analise = hinos_analise.set_index(\"numero\")\n",
    "hinos_analise[\"categoria_abr\"] = hinos_analise[\"categoria\"].apply(\n",
    "    lambda x: x[:13] + \"...\" if len(x) > 15 else x\n",
    ")\n",
    "hinos_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0d90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "# testar tokenizacao direto com spacy\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "stopwords.extend([\"ó\", \"ti\", \"pra\", \"lo\", \"oh\"])\n",
    "text = []\n",
    "text_no_stops = []\n",
    "\n",
    "for hino in tqdm(hinos_analise.to_dict(\"records\")):\n",
    "    tokens = nltk.tokenize.regexp_tokenize(hino[\"texto_limpo\"], r\"\\w+\")\n",
    "    # Replace \"MINH\" with \"MINHA\" with regex\n",
    "    tokens = [nltk.re.sub(r\"^minh$\", \"minha\", palavra.lower()) for palavra in tokens]\n",
    "\n",
    "    tokens_no_stops = [\n",
    "        palavra for palavra in tokens if palavra.lower() not in stopwords\n",
    "    ]\n",
    "    # remover pontuacao\n",
    "    tokens_no_stops = [palavra for palavra in tokens_no_stops if palavra.isalpha()]\n",
    "    text.append(tokens)\n",
    "    text_no_stops.append(tokens_no_stops)\n",
    "\n",
    "hinos_analise[\"tokens\"] = text\n",
    "hinos_analise[\"tokens_no_stops\"] = text_no_stops\n",
    "# considerando numero total de palavras, pois todas elas tem que ser cantadas, logo impactam no tamanho prático do hino\n",
    "hinos_analise[\"num_tokens\"] = hinos_analise[\"tokens\"].apply(len)\n",
    "hinos_analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(hinos_analise.sort_values(\"num_tokens\", ascending=False))\n",
    "display(hinos_analise.sort_values(\"num_tokens\", ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755eb4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'categoria_id' is treated as a categorical variable\n",
    "hinos_analise[\"categoria_id\"] = hinos_analise[\"categoria_id\"].astype(\"category\")\n",
    "\n",
    "# Create a mapping between categoria_id and categoria\n",
    "categoria_mapping = (\n",
    "    hinos_analise[[\"categoria_id\", \"categoria_abr\"]]\n",
    "    .drop_duplicates()\n",
    "    .set_index(\"categoria_id\")[\"categoria_abr\"]\n",
    ")\n",
    "\n",
    "# Create a violin plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\"\"\" sns.violinplot(\n",
    "    data=hinos_analise,\n",
    "    x=\"categoria_id\",\n",
    "    y=\"num_tokens\",\n",
    "    palette=\"viridis\",\n",
    "    inner=\"quartile\",\n",
    ") \"\"\"\n",
    "sns.boxplot(data=hinos_analise, x=\"categoria_id\", y=\"num_tokens\", palette=\"viridis\")\n",
    "\n",
    "# Replace x-ticks with corresponding 'categoria' names\n",
    "plt.xticks(\n",
    "    ticks=range(len(categoria_mapping)),\n",
    "    labels=categoria_mapping,\n",
    "    rotation=90,\n",
    "    ha=\"right\",\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Categoria\")\n",
    "plt.ylabel(\"Number of Tokens\")\n",
    "plt.title(\"Relationship Between Number of Tokens and Categoria (Violin Plot)\")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418aef02",
   "metadata": {},
   "source": [
    "## Word length (maybe)\n",
    "\n",
    "- bag-of-words with frequency + word map\n",
    "- gensim corpus\n",
    "- tf-idf\n",
    "- named entity recognition (spacy or polyglot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c228acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lines = hinos_analise.iloc[0][\"tokens_no_stops\"]\n",
    "tokenized_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a frequency list of lengths: line_num_words\n",
    "line_num_words = [len(t_line) for t_line in hinos_analise[\"tokens_no_stops\"].explode().tolist()]\n",
    "\n",
    "# Plot a histogram of the line lengths\n",
    "plt.hist(line_num_words)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f0ae9",
   "metadata": {},
   "source": [
    "## Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14540bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras = hinos_analise[\"tokens_no_stops\"].explode().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211aa20e",
   "metadata": {},
   "source": [
    "### Palavras mais longas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 10 largest words\n",
    "palavras_unique = list(set(palavras))\n",
    "palavras_unique.sort(key=len, reverse=True)\n",
    "print(len(palavras_unique))\n",
    "pd.DataFrame({\n",
    "    \"palavra\": palavras_unique[:10],\n",
    "    \"tamanho\": [len(palavra) for palavra in palavras_unique[:10]]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73d61b",
   "metadata": {},
   "source": [
    "### Bag-of-words with frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(palavras))\n",
    "set_words_full = list(set(palavras))\n",
    "count_words = [palavras.count(i) for i in set_words_full]\n",
    "\n",
    "contagem_palav = pd.DataFrame(\n",
    "    zip(set_words_full, count_words), columns=[\"palavra\", \"contagem\"]\n",
    ")\n",
    "contagem_palav = contagem_palav.sort_values(\"contagem\", ascending=False)\n",
    "contagem_palav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a percentage column\n",
    "contagem_palav[\"percentual\"] = contagem_palav[\"contagem\"] / len(palavras) * 100\n",
    "contagem_palav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba23669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a dictionary from the word frequency data\n",
    "word_freq_dict = dict(zip(contagem_palav['palavra'], contagem_palav['contagem']))\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud(\n",
    "    width=800, \n",
    "    height=400, \n",
    "    background_color='white',\n",
    "    max_words=100,\n",
    "    colormap='viridis',\n",
    "    relative_scaling=0.5,\n",
    "    random_state=42\n",
    ").generate_from_frequencies(word_freq_dict)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Palavras mais frequentes nos hinos', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also show top 20 most frequent words as a bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = contagem_palav.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['contagem'], color='skyblue')\n",
    "plt.yticks(range(len(top_20)), top_20['palavra'])\n",
    "plt.xlabel('Frequência')\n",
    "plt.title('Top 20 Palavras Mais Frequentes')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f2f13",
   "metadata": {},
   "source": [
    "~~## Named Entity Recognition~~\n",
    "\n",
    "Tentei:\n",
    "- Gensim corpus (problemas de compatibilidade)\n",
    "- NLTK NER (ruim)\n",
    "- Polyglot (não consegui instalar)\n",
    "- SpaCy (péssimos resultados): comparei tokenização + lematização com a anterior, prefiro a minha.\n",
    "\n",
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Exemplo: gerar bigramas do corpus inteiro\n",
    "def get_bigrams(tokens):\n",
    "    return list(nltk.ngrams(tokens, 2))  # 2 = bigramas\n",
    "\n",
    "\n",
    "# Gerar bigramas para todos os hinos\n",
    "hinos_analise[\"bigrams\"] = hinos_analise[\"tokens_no_stops\"].apply(get_bigrams)\n",
    "\n",
    "# Contar bigramas mais frequentes no corpus inteiro\n",
    "all_bigrams = [bigram for hino in hinos_analise[\"bigrams\"] for bigram in hino]\n",
    "bigram_freq = Counter(all_bigrams)\n",
    "\n",
    "bigram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b2bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: gerar trigramas do corpus inteiro\n",
    "def get_trigrams(tokens):\n",
    "    return list(nltk.ngrams(tokens, 3))  # 3 = trigrams\n",
    "\n",
    "\n",
    "# Gerar trigrams para todos os hinos\n",
    "hinos_analise[\"trigrams\"] = hinos_analise[\"tokens_no_stops\"].apply(get_trigrams)\n",
    "\n",
    "# Contar trigrams mais frequentes no corpus inteiro\n",
    "all_trigrams = [trigram for hino in hinos_analise[\"trigrams\"] for trigram in hino]\n",
    "trigram_freq = Counter(all_trigrams)\n",
    "\n",
    "trigram_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9224fead",
   "metadata": {},
   "source": [
    "## Matriz de frequencia e similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Juntar os tokens em string (CountVectorizer trabalha com texto)\n",
    "hinos_analise[\"tokens_str\"] = hinos_analise[\"tokens_no_stops\"].apply(lambda t: \" \".join(t))\n",
    "\n",
    "# Criar o vetor de frequências (aqui só unigramas)\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3)\n",
    ")  # unigramas e bigramas ngram_range=(1, 2)\n",
    "X = vectorizer.fit_transform(hinos_analise[\"tokens_str\"])\n",
    "\n",
    "# Similaridade de cosseno entre hinos\n",
    "similarity = cosine_similarity(X)\n",
    "similarity_df = pd.DataFrame(similarity, index=hinos_analise.index, columns=hinos_analise.index)\n",
    "similarity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b082682",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity = similarity_df[(similarity_df > 0.5) & (similarity_df < 1.0)].stack()#.reset_index()\n",
    "high_similarity = high_similarity[high_similarity.index.get_level_values(0) < high_similarity.index.get_level_values(1)]\n",
    "high_similarity.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204983c6",
   "metadata": {},
   "source": [
    "## Matriz com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# TF-IDF: unigrams e bigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_tfidf = vectorizer.fit_transform(hinos_analise[\"tokens_str\"])\n",
    "\n",
    "# DataFrame para visualizar\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_tfidf.toarray(), columns=vectorizer.get_feature_names_out(), index=hinos_analise.index\n",
    ")\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c828bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_for_hymn(row, features, top_n=5):\n",
    "    row_data = list(zip(features, row))\n",
    "    row_data = sorted(row_data, key=lambda x: x[1], reverse=True)\n",
    "    return row_data[:top_n]\n",
    "\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "for idx, row in enumerate(X_tfidf.toarray()[:5]): \n",
    "    top_terms = top_terms_for_hymn(row, features, top_n=5)\n",
    "    print(f\"\\n🎵 Hino {idx}:\")\n",
    "    for term, score in top_terms:\n",
    "        print(f\"  {term}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b8be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_tfidf = cosine_similarity(X_tfidf)\n",
    "\n",
    "similarity_df_tfidf = pd.DataFrame(similarity_tfidf, index=hinos_analise.index, columns=hinos_analise.index)\n",
    "\n",
    "similarity_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_df_tfidf, cmap=\"viridis\", annot=False)\n",
    "plt.title(\"Similaridade entre hinos (TF-IDF)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a92e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_similarity_tfidf = similarity_df_tfidf[\n",
    "    (similarity_df_tfidf > 0.5) & (similarity_df_tfidf < 1.0)\n",
    "].stack()  # .reset_index()\n",
    "high_similarity_tfidf = high_similarity_tfidf[\n",
    "    high_similarity_tfidf.index.get_level_values(0)\n",
    "    < high_similarity_tfidf.index.get_level_values(1)\n",
    "]\n",
    "high_similarity_tfidf.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c09f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hinos_analise.to_pickle(assets_folder / \"hinos_analise_tokens.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
